{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and functions.\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.ml.feature as ft\n",
    "import pyspark.ml.classification as cl\n",
    "import pyspark.ml.evaluation as ev\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.stat import ChiSquareTest\n",
    "import pyspark.ml.tuning as tune\n",
    "from pyspark.sql.functions import array, col, count, countDistinct, explode, isnan, lit, regexp_replace, split, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "|step|     customer|age|gender|zipcodeOri|     merchant|zipMerchant|           category|amount|fraud|\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "|   0|'C1093826151'|'4'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  4.55|    0|\n",
      "|   0| 'C352968107'|'2'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 39.68|    0|\n",
      "|   0|'C2054744914'|'4'|   'F'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 26.89|    0|\n",
      "|   0|'C1760612790'|'3'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 17.25|    0|\n",
      "|   0| 'C757503768'|'5'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 35.72|    0|\n",
      "|   0|'C1315400589'|'3'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 25.81|    0|\n",
      "|   0| 'C765155274'|'1'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|   9.1|    0|\n",
      "|   0| 'C202531238'|'4'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 21.17|    0|\n",
      "|   0| 'C105845174'|'3'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  32.4|    0|\n",
      "|   0|  'C39858251'|'5'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'|  35.4|    0|\n",
      "|   0|  'C98707741'|'4'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 14.95|    0|\n",
      "|   0|'C1551465414'|'1'|   'M'|   '28007'|'M1823072687'|    '28007'|'es_transportation'|  1.51|    0|\n",
      "|   0| 'C623601481'|'3'|   'M'|   '28007'|  'M50039827'|    '28007'|        'es_health'| 68.79|    0|\n",
      "|   0|'C1865204568'|'5'|   'M'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 20.32|    0|\n",
      "|   0| 'C490238464'|'3'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 13.56|    0|\n",
      "|   0| 'C194016923'|'3'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 30.19|    0|\n",
      "|   0|'C1207205377'|'4'|   'M'|   '28007'|'M1823072687'|    '28007'|'es_transportation'| 17.54|    0|\n",
      "|   0| 'C834963773'|'5'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 40.69|    0|\n",
      "|   0|'C1897705669'|'2'|   'M'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 21.21|    0|\n",
      "|   0| 'C124539163'|'2'|   'F'|   '28007'| 'M348934600'|    '28007'|'es_transportation'| 10.09|    0|\n",
      "+----+-------------+---+------+----------+-------------+-----------+-------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create and show a dataframe.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python\") \\\n",
    "    .getOrCreate()\n",
    "fraud_df = spark.read.csv('hdfs:///user/jkren001/bs140513_032310.csv', header=True, inferSchema=True)\n",
    "fraud_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- customer: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- zipcodeOri: string (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- zipMerchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- fraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema.\n",
    "fraud_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "594643"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of rows in the dataframe.\n",
    "fraud_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---+------+----------+--------+-----------+--------+------+-----+\n",
      "|step|customer|age|gender|zipcodeOri|merchant|zipMerchant|category|amount|fraud|\n",
      "+----+--------+---+------+----------+--------+-----------+--------+------+-----+\n",
      "|   0|       0|  0|     0|         0|       0|          0|       0|     0|    0|\n",
      "+----+--------+---+------+----------+--------+-----------+--------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check for the number of missing values in each column. No missing values. \n",
    "fraud_df.select([count(when(isnan(c), c)).alias(c) for c in fraud_df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+------+-------------+-----------+-----------+--------------------+------------------+-----+\n",
      "|step|   customer|  age|gender|zipcodeOrigin|   merchant|zipMerchant|merchandise category|transaction amount|fraud|\n",
      "+----+-----------+-----+------+-------------+-----------+-----------+--------------------+------------------+-----+\n",
      "|   0|C1093826151|46-55|  Male|        28007| M348934600|      28007|      transportation|              4.55|    0|\n",
      "|   0| C352968107|26-35|  Male|        28007| M348934600|      28007|      transportation|             39.68|    0|\n",
      "|   0|C2054744914|46-55|Female|        28007|M1823072687|      28007|      transportation|             26.89|    0|\n",
      "|   0|C1760612790|36-45|  Male|        28007| M348934600|      28007|      transportation|             17.25|    0|\n",
      "|   0| C757503768|56-65|  Male|        28007| M348934600|      28007|      transportation|             35.72|    0|\n",
      "|   0|C1315400589|36-45|Female|        28007| M348934600|      28007|      transportation|             25.81|    0|\n",
      "|   0| C765155274|19-25|Female|        28007| M348934600|      28007|      transportation|               9.1|    0|\n",
      "|   0| C202531238|46-55|Female|        28007| M348934600|      28007|      transportation|             21.17|    0|\n",
      "|   0| C105845174|36-45|  Male|        28007| M348934600|      28007|      transportation|              32.4|    0|\n",
      "|   0|  C39858251|56-65|Female|        28007| M348934600|      28007|      transportation|              35.4|    0|\n",
      "|   0|  C98707741|46-55|Female|        28007| M348934600|      28007|      transportation|             14.95|    0|\n",
      "|   0|C1551465414|19-25|  Male|        28007|M1823072687|      28007|      transportation|              1.51|    0|\n",
      "|   0| C623601481|36-45|  Male|        28007|  M50039827|      28007|              health|             68.79|    0|\n",
      "|   0|C1865204568|56-65|  Male|        28007|M1823072687|      28007|      transportation|             20.32|    0|\n",
      "|   0| C490238464|36-45|  Male|        28007| M348934600|      28007|      transportation|             13.56|    0|\n",
      "|   0| C194016923|36-45|Female|        28007| M348934600|      28007|      transportation|             30.19|    0|\n",
      "|   0|C1207205377|46-55|  Male|        28007|M1823072687|      28007|      transportation|             17.54|    0|\n",
      "|   0| C834963773|56-65|Female|        28007| M348934600|      28007|      transportation|             40.69|    0|\n",
      "|   0|C1897705669|26-35|  Male|        28007| M348934600|      28007|      transportation|             21.21|    0|\n",
      "|   0| C124539163|26-35|Female|        28007| M348934600|      28007|      transportation|             10.09|    0|\n",
      "+----+-----------+-----+------+-------------+-----------+-----------+--------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning. Rename some feature variables and their categories for clarity and remove quotation marks.\n",
    "fraud_df = fraud_df.withColumn('age', \\\n",
    "    when(fraud_df.age == \"'0'\", \"<=18\") \\\n",
    "    .when(fraud_df.age == \"'1'\", \"19-25\") \\\n",
    "    .when(fraud_df.age == \"'2'\", \"26-35\") \\\n",
    "    .when(fraud_df.age == \"'3'\", \"36-45\") \\\n",
    "    .when(fraud_df.age == \"'4'\", \"46-55\") \\\n",
    "    .when(fraud_df.age == \"'5'\", \"56-65\") \\\n",
    "    .when(fraud_df.age == \"'6'\", \">65\") \\\n",
    "    .when(fraud_df.age == \"'U'\", \"Unknown\")\n",
    "    .otherwise(fraud_df.age))\n",
    "fraud_df = fraud_df.withColumn('gender', \\\n",
    "    when(fraud_df.gender == \"'M'\", \"Male\") \\\n",
    "    .when(fraud_df.gender == \"'F'\", \"Female\") \\\n",
    "    .when(fraud_df.gender == \"'E'\", \"Enterprise\") \\\n",
    "    .when(fraud_df.gender == \"'U'\", \"Unknown\")\n",
    "    .otherwise(fraud_df.gender))\n",
    "fraud_df = fraud_df.withColumn('customer', regexp_replace('customer', \"'\", \"\"))\n",
    "fraud_df = fraud_df.withColumn('zipcodeOri', regexp_replace('zipcodeOri', \"'\", \"\"))\n",
    "fraud_df = fraud_df.withColumn('merchant', regexp_replace('merchant', \"'\", \"\"))\n",
    "fraud_df = fraud_df.withColumn('zipMerchant', regexp_replace('zipMerchant', \"'\", \"\"))\n",
    "fraud_df = fraud_df.withColumn('category', split(fraud_df.category, '\\_')[1])\n",
    "fraud_df = fraud_df.withColumn('category', split(fraud_df.category, \"'\")[0])\n",
    "fraud_df = fraud_df.withColumn('category', when(fraud_df.category == \"hyper\", \"hypermarkets\") \\\n",
    "                               .when(fraud_df.category == \"tech\", \"technology\")\n",
    "                               .otherwise(fraud_df.category))\n",
    "fraud_df = fraud_df.withColumnRenamed('zipcodeOri', 'zipcodeOrigin')\n",
    "fraud_df = fraud_df.withColumnRenamed('category', 'merchandise category')\n",
    "fraud_df = fraud_df.withColumnRenamed('amount', 'transaction amount')\n",
    "fraud_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(DISTINCT step)|\n",
      "+--------------------+\n",
      "|                 180|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct categories for step feature.\n",
    "fraud_df.select(countDistinct('step')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT customer)|\n",
      "+------------------------+\n",
      "|                    4112|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct categories for customer feature.\n",
    "fraud_df.select(countDistinct('customer')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|    age|\n",
      "+-------+\n",
      "|  19-25|\n",
      "|  26-35|\n",
      "|  46-55|\n",
      "|Unknown|\n",
      "|    >65|\n",
      "|  36-45|\n",
      "|  56-65|\n",
      "|   <=18|\n",
      "+-------+\n",
      "\n",
      "+-------------------+\n",
      "|count(DISTINCT age)|\n",
      "+-------------------+\n",
      "|                  8|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct categories for age feature and count their number.\n",
    "fraud_df.select('age').distinct().show()\n",
    "fraud_df.select(countDistinct('age')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|    gender|\n",
      "+----------+\n",
      "|    Female|\n",
      "|   Unknown|\n",
      "|      Male|\n",
      "|Enterprise|\n",
      "+----------+\n",
      "\n",
      "+----------------------+\n",
      "|count(DISTINCT gender)|\n",
      "+----------------------+\n",
      "|                     4|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct categories for gender feature and count their number.\n",
    "fraud_df.select('gender').distinct().show()\n",
    "fraud_df.select(countDistinct('gender')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|zipcodeOrigin|\n",
      "+-------------+\n",
      "|        28007|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zipcodeOrigin feature has only one distinct category.\n",
    "fraud_df.select('zipcodeOrigin').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|count(DISTINCT merchant)|\n",
      "+------------------------+\n",
      "|                      50|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count the number of distinct categories for merchant feature.\n",
    "fraud_df.select(countDistinct('merchant')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|zipMerchant|\n",
      "+-----------+\n",
      "|      28007|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# zipMerchant feature has only one distinct category.\n",
    "fraud_df.select('zipMerchant').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----+------+-----------+--------------------+------------------+-----+\n",
      "|step|   customer|  age|gender|   merchant|merchandise category|transaction amount|fraud|\n",
      "+----+-----------+-----+------+-----------+--------------------+------------------+-----+\n",
      "|   0|C1093826151|46-55|  Male| M348934600|      transportation|              4.55|    0|\n",
      "|   0| C352968107|26-35|  Male| M348934600|      transportation|             39.68|    0|\n",
      "|   0|C2054744914|46-55|Female|M1823072687|      transportation|             26.89|    0|\n",
      "|   0|C1760612790|36-45|  Male| M348934600|      transportation|             17.25|    0|\n",
      "|   0| C757503768|56-65|  Male| M348934600|      transportation|             35.72|    0|\n",
      "|   0|C1315400589|36-45|Female| M348934600|      transportation|             25.81|    0|\n",
      "|   0| C765155274|19-25|Female| M348934600|      transportation|               9.1|    0|\n",
      "|   0| C202531238|46-55|Female| M348934600|      transportation|             21.17|    0|\n",
      "|   0| C105845174|36-45|  Male| M348934600|      transportation|              32.4|    0|\n",
      "|   0|  C39858251|56-65|Female| M348934600|      transportation|              35.4|    0|\n",
      "|   0|  C98707741|46-55|Female| M348934600|      transportation|             14.95|    0|\n",
      "|   0|C1551465414|19-25|  Male|M1823072687|      transportation|              1.51|    0|\n",
      "|   0| C623601481|36-45|  Male|  M50039827|              health|             68.79|    0|\n",
      "|   0|C1865204568|56-65|  Male|M1823072687|      transportation|             20.32|    0|\n",
      "|   0| C490238464|36-45|  Male| M348934600|      transportation|             13.56|    0|\n",
      "|   0| C194016923|36-45|Female| M348934600|      transportation|             30.19|    0|\n",
      "|   0|C1207205377|46-55|  Male|M1823072687|      transportation|             17.54|    0|\n",
      "|   0| C834963773|56-65|Female| M348934600|      transportation|             40.69|    0|\n",
      "|   0|C1897705669|26-35|  Male| M348934600|      transportation|             21.21|    0|\n",
      "|   0| C124539163|26-35|Female| M348934600|      transportation|             10.09|    0|\n",
      "+----+-----------+-----+------+-----------+--------------------+------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove features zipcodeOrigin and zipMerchant as each of them has only one distinct category. \n",
    "fraud_df = fraud_df.drop('zipcodeOrigin', 'zipMerchant')\n",
    "fraud_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|merchandise category|\n",
      "+--------------------+\n",
      "|wellnessandbeauty   |\n",
      "|travel              |\n",
      "|leisure             |\n",
      "|sportsandtoys       |\n",
      "|hypermarkets        |\n",
      "|technology          |\n",
      "|barsandrestaurants  |\n",
      "|food                |\n",
      "|otherservices       |\n",
      "|health              |\n",
      "|hotelservices       |\n",
      "|home                |\n",
      "|contents            |\n",
      "|transportation      |\n",
      "|fashion             |\n",
      "+--------------------+\n",
      "\n",
      "+------------------------------------+\n",
      "|count(DISTINCT merchandise category)|\n",
      "+------------------------------------+\n",
      "|                                  15|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct categories for merchandise category feature and count their number.\n",
    "fraud_df.select('merchandise category').distinct().show(truncate=False)\n",
    "fraud_df.select(countDistinct('merchandise category')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|transaction amount|\n",
      "+-------+------------------+\n",
      "|  count|            594643|\n",
      "|   mean|37.890135308075436|\n",
      "| stddev|111.40283093084095|\n",
      "|    min|               0.0|\n",
      "|    max|           8329.96|\n",
      "+-------+------------------+\n",
      "\n",
      "+----------------------------+\n",
      "|skewness(transaction amount)|\n",
      "+----------------------------+\n",
      "|          32.365756507288985|\n",
      "+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the descriptive statistics and skewness for transaction amount feature. It is substantially positively skewed, \n",
    "# with the maximum value markedly exceeding the mean value.\n",
    "fraud_df.describe(['transaction amount']).show()\n",
    "fraud_df.agg({'transaction amount': 'skewness'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|fraud|\n",
      "+-----+\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show distinct categories for fraud (target variable).\n",
    "fraud_df.select('fraud').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|fraud| count|\n",
      "+-----+------+\n",
      "|    1|  7200|\n",
      "|    0|587443|\n",
      "+-----+------+\n",
      "\n",
      "The share of fraud transactions in the dataset is: 1.2%\n"
     ]
    }
   ],
   "source": [
    "# Group by fraud categories and the categories' frequencies and count the share of fraud transactions.\n",
    "fraud_df.groupby('fraud').count().show()\n",
    "print('The share of fraud transactions in the dataset is: {0:.1f}%'.format(fraud_df.filter(col(\"fraud\") == 1).count()/fraud_df.count()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...meaning that the dataset is highly imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "|merchandise category|avg(transaction amount)|\n",
      "+--------------------+-----------------------+\n",
      "|              travel|     2660.8028719723175|\n",
      "|                home|     457.48483443708596|\n",
      "|       hotelservices|      421.8233394160586|\n",
      "|          technology|      415.2741139240506|\n",
      "|              health|      407.0313384433963|\n",
      "|       sportsandtoys|     345.36681130171496|\n",
      "|       otherservices|     316.46960526315786|\n",
      "|             leisure|      300.2868776371309|\n",
      "|             fashion|     247.00818965517246|\n",
      "|   wellnessandbeauty|     229.42253481894176|\n",
      "|        hypermarkets|     169.25542857142855|\n",
      "|  barsandrestaurants|     164.09266666666662|\n",
      "+--------------------+-----------------------+\n",
      "\n",
      "+--------------------+-----------------------+\n",
      "|merchandise category|avg(transaction amount)|\n",
      "+--------------------+-----------------------+\n",
      "|              travel|      669.0255333333333|\n",
      "|                home|     113.33840855106888|\n",
      "|       hotelservices|     106.54854515050165|\n",
      "|              health|     103.73722795594652|\n",
      "|          technology|       99.9246383363471|\n",
      "|       sportsandtoys|       88.5027376237623|\n",
      "|       otherservices|      75.68549707602338|\n",
      "|             leisure|                73.2304|\n",
      "|             fashion|     62.347674345219204|\n",
      "|   wellnessandbeauty|       57.3202185412026|\n",
      "|            contents|     44.547570621468935|\n",
      "|  barsandrestaurants|     41.145997121381725|\n",
      "|        hypermarkets|      40.03714506703327|\n",
      "|                food|     37.070404890683285|\n",
      "|      transportation|      26.95818700147843|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the average amounts spent in fraud and non-fraud transactions for each category.\n",
    "fraud_df.filter(fraud_df.fraud == 1).groupby('merchandise category').mean().orderBy('avg(transaction amount)', ascending=False).select('merchandise category','avg(transaction amount)').show() \n",
    "fraud_df.filter(fraud_df.fraud == 0).groupby('merchandise category').mean().orderBy('avg(transaction amount)', ascending=False).select('merchandise category','avg(transaction amount)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Travel is the top category in terms of the amounts spent both in a fraud and a non-fraud transaction on average. In addition, one can also see that the amount spent in a fraud transaction is on average around four times bigger than that in a non-fraud transaction for all categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|merchandise category|          avg(fraud)|\n",
      "+--------------------+--------------------+\n",
      "|             leisure|  0.9498997995991983|\n",
      "|              travel|  0.7939560439560439|\n",
      "|       sportsandtoys| 0.49525237381309345|\n",
      "|       hotelservices| 0.31422018348623854|\n",
      "|       otherservices|                0.25|\n",
      "|                home| 0.15206445115810674|\n",
      "|              health| 0.10512613896981343|\n",
      "|          technology| 0.06666666666666667|\n",
      "|   wellnessandbeauty| 0.04759379557205356|\n",
      "|        hypermarkets|0.045916693998032145|\n",
      "|  barsandrestaurants|0.018829436686019142|\n",
      "|             fashion|0.017973349860551595|\n",
      "|                food|                 0.0|\n",
      "|            contents|                 0.0|\n",
      "|      transportation|                 0.0|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the categories where fraud happens most frequently.\n",
    "fraud_df.groupby('merchandise category').mean().orderBy('avg(fraud)', ascending=False).select('merchandise category', 'avg(fraud)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leisure and travel are the categories where fraud happens most frequently: 95.0% and 79.4% of total category transactions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|    gender|          avg(fraud)|\n",
      "+----------+--------------------+\n",
      "|    Female|0.014659621339331104|\n",
      "|      Male| 0.00907278722730406|\n",
      "|Enterprise|0.005942275042444821|\n",
      "|   Unknown|                 0.0|\n",
      "+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the shares of men and women participating in fraud transactions.\n",
    "fraud_df.groupby('gender').mean().orderBy('avg(fraud)', ascending=False).select('gender','avg(fraud)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, 1.5% of women and 0.9% of men participate in fraud transactions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|    gender|avg(transaction amount)|\n",
      "+----------+-----------------------+\n",
      "|      Male|      540.3700780287468|\n",
      "|    Female|      526.1781883144178|\n",
      "|Enterprise|      473.4585714285714|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the average transaction amounts for men and women in fraud transactions.\n",
    "fraud_df.filter(fraud_df.fraud == 1).groupby('gender').mean().orderBy('avg(transaction amount)', ascending=False).select('gender','avg(transaction amount)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but men spend slightly more in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|    age|          avg(fraud)|\n",
      "+-------+--------------------+\n",
      "|   <=18| 0.01957585644371941|\n",
      "|  46-55| 0.01293281357486815|\n",
      "|  26-35| 0.01251401420105707|\n",
      "|  36-45|0.011928145666107075|\n",
      "|  19-25| 0.01185253995286508|\n",
      "|  56-65|0.010951119057501357|\n",
      "|    >65| 0.00974826324045716|\n",
      "|Unknown|0.005942275042444821|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare the shares of age categories participating in fraud transactions.\n",
    "fraud_df.groupby('age').mean().orderBy('avg(fraud)', ascending=False).select('age','avg(fraud)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2% of the youngest (18-year-olds and younger) participate in fraud transactions - the highest share among age categories... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+\n",
      "|    age|avg(transaction amount)|\n",
      "+-------+-----------------------+\n",
      "|   <=18|      657.2781249999999|\n",
      "|  26-35|      552.2124317406141|\n",
      "|    >65|       545.402681992337|\n",
      "|  36-45|      532.4287578347573|\n",
      "|  46-55|      522.4009148936171|\n",
      "|  19-25|     499.75769230769237|\n",
      "|  56-65|      489.4180174927113|\n",
      "|Unknown|      473.4585714285714|\n",
      "+-------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compare transaction amounts among the age categories participating in fraud transactions. \n",
    "fraud_df.filter(fraud_df.fraud == 1).groupby('age').mean().orderBy('avg(transaction amount)', ascending=False).select('age','avg(transaction amount)').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and spend more in a fraud transaction on average than other age categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------------+-----------+----------------+----------------+----------------+--------------+\n",
      "|transaction amount|fraud|step_indexed|age_indexed|category_indexed|merchant_indexed|customer_indexed|gender_indexed|\n",
      "+------------------+-----+------------+-----------+----------------+----------------+----------------+--------------+\n",
      "|              4.55|    0|       178.0|        2.0|             0.0|             1.0|          1795.0|           1.0|\n",
      "|             39.68|    0|       178.0|        0.0|             0.0|             1.0|          1620.0|           1.0|\n",
      "|             26.89|    0|       178.0|        2.0|             0.0|             0.0|          3796.0|           0.0|\n",
      "|             17.25|    0|       178.0|        1.0|             0.0|             1.0|          1273.0|           1.0|\n",
      "|             35.72|    0|       178.0|        3.0|             0.0|             1.0|          2814.0|           1.0|\n",
      "|             25.81|    0|       178.0|        1.0|             0.0|             1.0|           623.0|           0.0|\n",
      "|               9.1|    0|       178.0|        4.0|             0.0|             1.0|           586.0|           0.0|\n",
      "|             21.17|    0|       178.0|        2.0|             0.0|             1.0|           987.0|           0.0|\n",
      "|              32.4|    0|       178.0|        1.0|             0.0|             1.0|          3407.0|           1.0|\n",
      "|              35.4|    0|       178.0|        3.0|             0.0|             1.0|          2765.0|           0.0|\n",
      "|             14.95|    0|       178.0|        2.0|             0.0|             1.0|          2660.0|           0.0|\n",
      "|              1.51|    0|       178.0|        4.0|             0.0|             0.0|          2202.0|           1.0|\n",
      "|             68.79|    0|       178.0|        1.0|             2.0|            19.0|          3547.0|           1.0|\n",
      "|             20.32|    0|       178.0|        3.0|             0.0|             0.0|          1593.0|           1.0|\n",
      "|             13.56|    0|       178.0|        1.0|             0.0|             1.0|           864.0|           1.0|\n",
      "|             30.19|    0|       178.0|        1.0|             0.0|             1.0|          2392.0|           0.0|\n",
      "|             17.54|    0|       178.0|        2.0|             0.0|             0.0|          1678.0|           1.0|\n",
      "|             40.69|    0|       178.0|        3.0|             0.0|             1.0|            59.0|           0.0|\n",
      "|             21.21|    0|       178.0|        0.0|             0.0|             1.0|          1436.0|           1.0|\n",
      "|             10.09|    0|       178.0|        0.0|             0.0|             1.0|          3698.0|           0.0|\n",
      "+------------------+-----+------------+-----------+----------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Index categorical features' values with StringIndexer, then remove original columns.\n",
    "indexer = ft.StringIndexer(inputCols=['step', 'customer', 'age', 'gender', 'merchant', 'merchandise category'],\n",
    " outputCols=['step_indexed', 'customer_indexed', 'age_indexed', 'gender_indexed', 'merchant_indexed', 'category_indexed'])\n",
    "fraud_df_indexed = indexer.fit(fraud_df).transform(fraud_df)\n",
    "fraud_df_indexed = fraud_df_indexed.drop('step', 'customer', 'age', 'gender', 'merchant', 'merchandise category')\n",
    "fraud_df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features’ association with fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I set the null hypothesis that categorical features are not associated with fraud and will use Chi-square test to determine that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch VectorAssembler for calculating p-values of categorical features (statistical significance of association \n",
    "# between them and fraud). First, exclude the transaction amount feature as it is numerical.\n",
    "fraud_df_categorical = fraud_df_indexed.drop('transaction amount')\n",
    "targetv = ['fraud']\n",
    "\n",
    "assembler_chisq = ft.VectorAssembler(\n",
    "    inputCols=[x for x in fraud_df_categorical.columns if x not in targetv],\n",
    "    outputCol='features')\n",
    "\n",
    "# Convert to vector column.\n",
    "df_vector = assembler_chisq.transform(fraud_df_categorical).select('fraud', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-values:[0.9999999356451084,2.0021179258922217e-07,0.0,0.0,0.0,0.0]\n"
     ]
    }
   ],
   "source": [
    "# Run Chi-square test.  \n",
    "chi = ChiSquareTest.test(df_vector, 'features', 'fraud').head()\n",
    "print(\"p-values:\" + str(chi.pValues))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the step feature has p-value very close to 1, meaning that at almost 100% confidence level it is not associated with fraud. For other features, I reject the null hypothesis of no association with fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The point biserial correlation coefficient between transaction amount and fraud is: 0.49%\n"
     ]
    }
   ],
   "source": [
    "# Calculate point biserial correlation (equivalent to Pearson correlation) between transaction amount and fraud.\n",
    "print('The point biserial correlation coefficient between transaction amount and fraud is: {0:.2f}%'.format(fraud_df_indexed.corr('transaction amount', 'fraud')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...indicating moderate positive correlation. Therefore, I only remove the step variable from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "|transaction amount|fraud|age_indexed|category_indexed|merchant_indexed|customer_indexed|gender_indexed|\n",
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "|              4.55|    0|        2.0|             0.0|             1.0|          1795.0|           1.0|\n",
      "|             39.68|    0|        0.0|             0.0|             1.0|          1620.0|           1.0|\n",
      "|             26.89|    0|        2.0|             0.0|             0.0|          3796.0|           0.0|\n",
      "|             17.25|    0|        1.0|             0.0|             1.0|          1273.0|           1.0|\n",
      "|             35.72|    0|        3.0|             0.0|             1.0|          2814.0|           1.0|\n",
      "|             25.81|    0|        1.0|             0.0|             1.0|           623.0|           0.0|\n",
      "|               9.1|    0|        4.0|             0.0|             1.0|           586.0|           0.0|\n",
      "|             21.17|    0|        2.0|             0.0|             1.0|           987.0|           0.0|\n",
      "|              32.4|    0|        1.0|             0.0|             1.0|          3407.0|           1.0|\n",
      "|              35.4|    0|        3.0|             0.0|             1.0|          2765.0|           0.0|\n",
      "|             14.95|    0|        2.0|             0.0|             1.0|          2660.0|           0.0|\n",
      "|              1.51|    0|        4.0|             0.0|             0.0|          2202.0|           1.0|\n",
      "|             68.79|    0|        1.0|             2.0|            19.0|          3547.0|           1.0|\n",
      "|             20.32|    0|        3.0|             0.0|             0.0|          1593.0|           1.0|\n",
      "|             13.56|    0|        1.0|             0.0|             1.0|           864.0|           1.0|\n",
      "|             30.19|    0|        1.0|             0.0|             1.0|          2392.0|           0.0|\n",
      "|             17.54|    0|        2.0|             0.0|             0.0|          1678.0|           1.0|\n",
      "|             40.69|    0|        3.0|             0.0|             1.0|            59.0|           0.0|\n",
      "|             21.21|    0|        0.0|             0.0|             1.0|          1436.0|           1.0|\n",
      "|             10.09|    0|        0.0|             0.0|             1.0|          3698.0|           0.0|\n",
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove step feature from the dataframe.\n",
    "fraud_df_indexed = fraud_df_indexed.drop('step_indexed')\n",
    "fraud_df_indexed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch VectorAssembler for modelling (this time I include the amount feature).\n",
    "targetv = ['fraud']\n",
    "\n",
    "assembler_indexed = ft.VectorAssembler(\n",
    "    inputCols=[x for x in fraud_df_indexed.columns if x not in targetv],\n",
    "    outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into random train and test sets at 70%/30% with a fixed seed of 50.\n",
    "fraud_train, fraud_test = fraud_df_indexed.randomSplit([0.7, 0.3], seed=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratio: 81\n"
     ]
    }
   ],
   "source": [
    "# The issue with the imbalanced dataset needs to be resolved, so try both oversampling (populating fraud samples) \n",
    "# and undersampling (reducing non-fraud samples) in our train set and then compare the classifiers' performances. \n",
    "# First, calculate the ratio of non-fraud rows to fraud rows in our train set. \n",
    "major_fraud_train = fraud_train.filter(col(\"fraud\") == 0)\n",
    "minor_fraud_train = fraud_train.filter(col(\"fraud\") == 1)\n",
    "ratio = int(major_fraud_train.count()/minor_fraud_train.count())\n",
    "print(\"ratio: {}\".format(ratio))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "|transaction amount|fraud|age_indexed|category_indexed|merchant_indexed|customer_indexed|gender_indexed|\n",
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "|               0.0|    0|        0.0|             0.0|             0.0|           727.0|           0.0|\n",
      "|               0.0|    0|        0.0|             0.0|             0.0|          1101.0|           0.0|\n",
      "|               0.0|    0|        0.0|             0.0|             0.0|          2113.0|           1.0|\n",
      "|               0.0|    0|        0.0|             0.0|             0.0|          2818.0|           0.0|\n",
      "|               0.0|    0|        0.0|             0.0|             0.0|          2982.0|           1.0|\n",
      "|               0.0|    0|        0.0|             0.0|             1.0|           321.0|           0.0|\n",
      "|               0.0|    0|        0.0|             0.0|             1.0|          1440.0|           0.0|\n",
      "|               0.0|    0|        0.0|             0.0|             1.0|          1934.0|           0.0|\n",
      "|               0.0|    0|        0.0|             1.0|             2.0|          1883.0|           1.0|\n",
      "|               0.0|    0|        1.0|             0.0|             0.0|           115.0|           1.0|\n",
      "|               0.0|    0|        1.0|             0.0|             0.0|           390.0|           0.0|\n",
      "|               0.0|    0|        1.0|             0.0|             0.0|           391.0|           1.0|\n",
      "|               0.0|    0|        1.0|             0.0|             0.0|          1566.0|           1.0|\n",
      "|               0.0|    0|        1.0|             0.0|             0.0|          3375.0|           0.0|\n",
      "|               0.0|    0|        1.0|             0.0|             1.0|           894.0|           0.0|\n",
      "|               0.0|    0|        1.0|             0.0|             1.0|          1494.0|           0.0|\n",
      "|               0.0|    0|        2.0|             0.0|             0.0|           393.0|           1.0|\n",
      "|               0.0|    0|        2.0|             0.0|             1.0|          2252.0|           0.0|\n",
      "|               0.0|    0|        2.0|             0.0|             1.0|          2309.0|           0.0|\n",
      "|               0.0|    0|        2.0|             0.0|             1.0|          2338.0|           0.0|\n",
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now prepare the oversampled dataframe by populating fraud samples.\n",
    "a = range(ratio)\n",
    "oversamp_fraud_train = minor_fraud_train.withColumn('dummy', explode(array([lit(x) for x in a]))).drop('dummy')\n",
    "# Combine both oversampled fraud rows and unchanged non-fraud rows. \n",
    "combined_oversamp_fraud_train = major_fraud_train.unionAll(oversamp_fraud_train)\n",
    "combined_oversamp_fraud_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "|transaction amount|fraud|age_indexed|category_indexed|merchant_indexed|customer_indexed|gender_indexed|\n",
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "|               0.0|    0|        1.0|             0.0|             1.0|          1494.0|           0.0|\n",
      "|              0.05|    0|        0.0|             0.0|             0.0|           342.0|           0.0|\n",
      "|              0.05|    0|        2.0|             0.0|             0.0|          3026.0|           1.0|\n",
      "|              0.07|    0|        0.0|             0.0|             1.0|           766.0|           0.0|\n",
      "|              0.14|    0|        0.0|             0.0|             0.0|          2413.0|           1.0|\n",
      "|              0.16|    0|        4.0|             1.0|             2.0|           659.0|           1.0|\n",
      "|              0.19|    0|        4.0|             0.0|             1.0|          1443.0|           0.0|\n",
      "|               0.2|    0|        4.0|             0.0|             0.0|           928.0|           1.0|\n",
      "|              0.23|    0|        4.0|             0.0|             0.0|          2494.0|           0.0|\n",
      "|              0.24|    0|        2.0|             0.0|             0.0|          1901.0|           1.0|\n",
      "|              0.25|    0|        4.0|             0.0|             1.0|          2876.0|           0.0|\n",
      "|              0.27|    0|        4.0|             0.0|             1.0|          2215.0|           1.0|\n",
      "|              0.28|    0|        0.0|             0.0|             0.0|           832.0|           0.0|\n",
      "|              0.28|    0|        3.0|             0.0|             0.0|          3203.0|           1.0|\n",
      "|              0.28|    0|        4.0|             0.0|             1.0|          3452.0|           0.0|\n",
      "|              0.32|    0|        2.0|             0.0|             1.0|           889.0|           0.0|\n",
      "|              0.34|    0|        2.0|             0.0|             0.0|           570.0|           1.0|\n",
      "|              0.36|    0|        2.0|             0.0|             1.0|          2452.0|           1.0|\n",
      "|              0.39|    0|        2.0|             0.0|             1.0|          1399.0|           0.0|\n",
      "|              0.42|    0|        2.0|             0.0|             0.0|          1789.0|           1.0|\n",
      "+------------------+-----+-----------+----------------+----------------+----------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now prepare the undersampled dataframe by reducing non-fraud samples.\n",
    "undersamp_fraud_train = major_fraud_train.sample(False, 1/ratio, seed=50)\n",
    "# Combine both undersampled non-fraud rows and unchanged fraud rows.\n",
    "combined_undersamp_fraud_train = undersamp_fraud_train.unionAll(minor_fraud_train)\n",
    "combined_undersamp_fraud_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a random forest classifier, with maxBins at 4200 as the number of distinct categories \n",
    "# for customer_indexed feature is 4112, and a pipeline. Specify the label column, set seed at 50, \n",
    "# leave other parameters at default values. Then fit the classifier on the oversampled and undersampled train dataframes. \n",
    "classifierRF = cl.RandomForestClassifier(labelCol='fraud', maxBins=4200, seed=50)\n",
    "pipelineRF = Pipeline(stages=[assembler_indexed, classifierRF])\n",
    "model_oversampRF = pipelineRF.fit(combined_oversamp_fraud_train)\n",
    "model_undersampRF = pipelineRF.fit(combined_undersamp_fraud_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a logistic regression classifier and a pipeline. Specify the label column, leave other parameters at default values.\n",
    "# Then fit the classifier on the oversampled and undersampled train dataframes.\n",
    "classifierLR = cl.LogisticRegression(labelCol='fraud')\n",
    "pipelineLR = Pipeline(stages=[assembler_indexed, classifierLR])\n",
    "model_oversampLR = pipelineLR.fit(combined_oversamp_fraud_train)\n",
    "model_undersampLR = pipelineLR.fit(combined_undersamp_fraud_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification evaluators: binary for area under PR metric and multiclass for accuracy metric.\n",
    "binary_evaluator = ev.BinaryClassificationEvaluator(labelCol='fraud')\n",
    "multi_evaluator = ev.MulticlassClassificationEvaluator(labelCol='fraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR, random forest: 0.778\n",
      "Accuracy, random forest: 0.985\n",
      "Area under PR, logistic regression: 0.687\n",
      "Accuracy, logistic regression: 0.959\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate the classifiers fit on the oversampled train dataframes.\n",
    "test_model_oversampRF = model_oversampRF.transform(fraud_test)\n",
    "test_model_oversampLR = model_oversampLR.transform(fraud_test)\n",
    "print('Area under PR, random forest: {0:.3f}'.format(binary_evaluator.evaluate(test_model_oversampRF, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, random forest: {0:.3f}'.format(multi_evaluator.evaluate(test_model_oversampRF, {multi_evaluator.metricName: 'accuracy'})))\n",
    "print('Area under PR, logistic regression: {0:.3f}'.format(binary_evaluator.evaluate(test_model_oversampLR, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, logistic regression: {0:.3f}'.format(multi_evaluator.evaluate(test_model_oversampLR, {multi_evaluator.metricName: 'accuracy'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest classifier outperformed the logistic regression classifier on the oversampled train dataframe, in particular in the area under PR metric. The logistic regression classifier gave unreasonable preference to higher labels of ordinally encoded variables, leading to bias and poorer model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR, random forest: 0.670\n",
      "Accuracy, random forest: 0.951\n",
      "Area under PR, logistic regression: 0.682\n",
      "Accuracy, logistic regression: 0.957\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate the classifiers fit on the undersampled train dataframes.\n",
    "test_model_undersampRF = model_undersampRF.transform(fraud_test)\n",
    "test_model_undersampLR = model_undersampLR.transform(fraud_test)\n",
    "print('Area under PR, random forest: {0:.3f}'.format(binary_evaluator.evaluate(test_model_undersampRF, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, random forest: {0:.3f}'.format(multi_evaluator.evaluate(test_model_undersampRF, {multi_evaluator.metricName: 'accuracy'})))\n",
    "print('Area under PR, logistic regression: {0:.3f}'.format(binary_evaluator.evaluate(test_model_undersampLR, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, logistic regression: {0:.3f}'.format(multi_evaluator.evaluate(test_model_undersampLR, {multi_evaluator.metricName: 'accuracy'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, both classifiers registered lower scores in both metrics when fit on the undersampled train dataframe. Undersampling probably resulted in the loss of valuable information about the majority class, with the chosen sample not accurately representing the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the logistic regression classifier gave undue preference to higher labels of ordinally encoded variables, \n",
    "# encode the categorical features with one-hot encoder and evalute models again.\n",
    "\n",
    "encoder = ft.OneHotEncoder(inputCols=['customer_indexed', 'age_indexed', 'gender_indexed', 'merchant_indexed', 'category_indexed'],\n",
    " outputCols=['customer_vec', 'age_vec', 'gender_vec', 'merchant_vec', 'category_vec'])\n",
    "encoded = encoder.fit(fraud_df_indexed).transform(fraud_df_indexed)\n",
    "assembler_encoded = ft.VectorAssembler(inputCols = ['customer_vec', 'age_vec', 'gender_vec', 'merchant_vec', 'category_vec'], \n",
    "                                        outputCol = 'features')\n",
    "\n",
    "# Remove indexed features from our dataframe.\n",
    "fraud_df_encoded = encoded.drop('age_indexed', 'category_indexed', 'merchant_indexed', 'customer_indexed', 'gender_indexed')\n",
    "\n",
    "# Split the dataframe into random train and test sets at 70%/30% with a fixed seed.\n",
    "fraud_train_e, fraud_test_e = fraud_df_encoded.randomSplit([0.7, 0.3], seed=50)\n",
    "\n",
    "# Again, the issue with the imbalanced dataset has to be resolved, so try both oversampling (populating fraud samples) \n",
    "# and undersampling (reducing non-fraud samples) in the train set and then compare the classifiers' performances. \n",
    "# First, calculate the ratio of non-fraud rows to fraud rows in the train set. \n",
    "major_fraud_train_e = fraud_train_e.filter(col(\"fraud\") == 0)\n",
    "minor_fraud_train_e = fraud_train_e.filter(col(\"fraud\") == 1)\n",
    "ratio_e = int(major_fraud_train_e.count()/minor_fraud_train_e.count())\n",
    "\n",
    "# Prepare the oversampled dataframe by populating fraud samples.\n",
    "a_e = range(ratio_e)\n",
    "oversamp_fraud_train_e = minor_fraud_train_e.withColumn('dummy', explode(array([lit(x) for x in a_e]))).drop('dummy')\n",
    "\n",
    "# Combine both oversampled fraud rows and unchanged non-fraud rows. \n",
    "combined_oversamp_fraud_train_e = major_fraud_train_e.unionAll(oversamp_fraud_train_e)\n",
    "\n",
    "# Now prepare the undersampled dataframe by reducing non-fraud samples.\n",
    "undersamp_fraud_train_e = major_fraud_train_e.sample(False, 1/ratio_e, seed=50)\n",
    "\n",
    "# Combine both undersampled non-fraud rows and unchanged fraud rows.\n",
    "combined_undersamp_fraud_train_e = undersamp_fraud_train_e.unionAll(minor_fraud_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline, then fit the random forest classifier on the oversampled and undersampled train dataframes.\n",
    "pipelineRF_e = Pipeline(stages=[assembler_encoded, classifierRF])\n",
    "model_oversampRF_e = pipelineRF_e.fit(combined_oversamp_fraud_train_e)\n",
    "model_undersampRF_e = pipelineRF_e.fit(combined_undersamp_fraud_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline, then fit the logistic regression classifier on the oversampled and undersampled train dataframes.\n",
    "pipelineLR_e = Pipeline(stages=[assembler_encoded, classifierLR])\n",
    "model_oversampLR_e = pipelineLR_e.fit(combined_oversamp_fraud_train_e)\n",
    "model_undersampLR_e = pipelineLR_e.fit(combined_undersamp_fraud_train_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR, random forest: 0.428\n",
      "Accuracy, random forest: 0.882\n",
      "Area under PR, logistic regression: 0.765\n",
      "Accuracy, logistic regression: 0.986\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate the classifiers fit on the oversampled train dataframes.\n",
    "test_model_oversampRF_e = model_oversampRF_e.transform(fraud_test_e)\n",
    "test_model_oversampLR_e = model_oversampLR_e.transform(fraud_test_e)\n",
    "print('Area under PR, random forest: {0:.3f}'.format(binary_evaluator.evaluate(test_model_oversampRF_e, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, random forest: {0:.3f}'.format(multi_evaluator.evaluate(test_model_oversampRF_e, {multi_evaluator.metricName: 'accuracy'})))\n",
    "print('Area under PR, logistic regression: {0:.3f}'.format(binary_evaluator.evaluate(test_model_oversampLR_e, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, logistic regression: {0:.3f}'.format(multi_evaluator.evaluate(test_model_oversampLR_e, {multi_evaluator.metricName: 'accuracy'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression classifier outperformed the random forest classifier, especially in the area under PR metric, on the oversampled train dataframe after one-hot encoding. While the logistic regression classifier’s performance improved in both metrics, the one of the random forest classifier worsened as one-hot encoding led to a marked increase in dimensionality because of a large number of categories for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under PR, random forest: 0.397\n",
      "Accuracy, random forest: 0.936\n",
      "Area under PR, logistic regression: 0.583\n",
      "Accuracy, logistic regression: 0.953\n"
     ]
    }
   ],
   "source": [
    "# Test and evaluate the classifiers fit on the undersampled train dataframes.\n",
    "test_model_undersampRF_e = model_undersampRF_e.transform(fraud_test_e)\n",
    "test_model_undersampLR_e = model_undersampLR_e.transform(fraud_test_e)\n",
    "print('Area under PR, random forest: {0:.3f}'.format(binary_evaluator.evaluate(test_model_undersampRF_e, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, random forest: {0:.3f}'.format(multi_evaluator.evaluate(test_model_undersampRF_e, {multi_evaluator.metricName: 'accuracy'})))\n",
    "print('Area under PR, logistic regression: {0:.3f}'.format(binary_evaluator.evaluate(test_model_undersampLR_e, {binary_evaluator.metricName: 'areaUnderPR'})))\n",
    "print('Accuracy, logistic regression: {0:.3f}'.format(multi_evaluator.evaluate(test_model_undersampLR_e, {multi_evaluator.metricName: 'accuracy'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After one-hot encoding, the logistic regression classifier outperformed the random forest classifier in both metrics on the undersampled train dataframe too. But both classifiers again recorded lower scores in the area under PR metric than the ones fit on the oversampled train dataframes. The reason probably was the same as in the case with numerical indexing of categorical features: loss of valuable majority class data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression classifier trained on the oversampled train dataframe after one-hot encoding, I use grid search to choose the best two parameters. I try the maximum number of iterations at 50 and 100 and the regularisation parameter at 0 and 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search for choosing the best parameters for a logistic regression classifier.\n",
    "gridLR = tune.ParamGridBuilder() \\\n",
    " .addGrid(classifierLR.maxIter, \n",
    " [50, 100]) \\\n",
    " .addGrid(classifierLR.regParam, \n",
    " [0, 0.01]) \\\n",
    " .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best area under PR, logistic regression: 0.766\n"
     ]
    }
   ],
   "source": [
    "# Launch the crossvalidator.\n",
    "cvLR = tune.CrossValidator(estimator=pipelineLR_e, estimatorParamMaps=gridLR, evaluator=binary_evaluator)\n",
    "# Test and evaluate the logistic regression classifier fit on the oversampled dataframe again.\n",
    "cvModelLR = cvLR.fit(combined_oversamp_fraud_train_e)\n",
    "results_cvLR = cvModelLR.transform(fraud_test_e)\n",
    "print('Best area under PR, logistic regression: {0:.3f}'.format(binary_evaluator.evaluate(results_cvLR, {binary_evaluator.metricName: 'areaUnderPR'})))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can observe a small improvement in the logistic regression classifier's performance in the area under PR metric: to 0.766 from 0.765."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam), logistic regression:  0.0\n",
      "Best Param (MaxIter), logistic regression:  50\n"
     ]
    }
   ],
   "source": [
    "# Print the best parameters. \n",
    "print('Best Param (regParam), logistic regression: ', cvModelLR.bestModel.stages[-1]._java_obj.parent().getRegParam())\n",
    "print('Best Param (MaxIter), logistic regression: ', cvModelLR.bestModel.stages[-1]._java_obj.parent().getMaxIter())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best maximum number of iterations is 50 rather than the default 100."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lopez-Rojas, E.A. Synthetic data from a financial payment system [Online]. Available from: https://www.kaggle.com/datasets/ealaxi/banksim1 [11 September 2022].\n",
    "* Avci, T. Fraud Detection on Bank Payments [Online]. Available from: https://www.kaggle.com/code/turkayavci/fraud-detection-on-bank-payments [11 September 2022].\n",
    "* Drabas, T., Lee, D. (2017). Learning PySpark. Birmingham - Mumbai: Packt Publishing.\n",
    "* DeJesus, J. (2019). Point Biserial Correlation with Python [Online]. Available from: https://towardsdatascience.com/point-biserial-correlation-with-python-f7cd591bd3b1 [11 September 2022].\n",
    "* Wan, J. (2020). Oversampling and Undersampling with PySpark [Online]. Available from: https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253 [11 September 2022].\n",
    "* GeeksforGeeks (2022). ML | One Hot Encoding to treat Categorical data parameters [Online]. Available from: https://www.geeksforgeeks.org/ml-one-hot-encoding-of-datasets-in-python/ [11 September 2022].\n",
    "* SparkByExamples.Com. (2022). PySpark When Otherwise | SQL Case When Usage [Online]. Available from: https://sparkbyexamples.com/pyspark/pyspark-when-otherwise/ [11 September 2022].\n",
    "* SparkByExamples.Com. (2022). PySpark Replace Column Values in DataFrame [Online]. Available from: https://sparkbyexamples.com/pyspark/pyspark-replace-column-values/ [11 September 2022].\n",
    "* Stack Overflow. How to find count of Null and Nan values for each column in a PySpark dataframe efficiently? [Online]. Available from: https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe [11 September 2022].\n",
    "* Stack Overflow. How to delete specific characters from a string in a PySpark dataframe? [Online]. Available from: https://stackoverflow.com/questions/66141218/how-to-delete-specific-characters-from-a-string-in-a-pyspark-dataframe [11 September 2022].\n",
    "* Stack Overflow. pyspark p values and chisquaretest correlations [Online]. Available from: https://stackoverflow.com/questions/59055704/pyspark-p-values-and-chisquaretest-correlations [11 September 2022].\n",
    "* Stack Overflow. How to use multiple columns in filter and lambda functions pyspark [Online]. Available from: https://stackoverflow.com/questions/60400741/how-to-use-multiple-columns-in-filter-and-lambda-functions-pyspark [11 September 2022].\n",
    "* Stack Overflow. How do you perform one hot encoding with PySpark [Online]. Available from: https://stackoverflow.com/questions/55922787/how-do-you-perform-one-hot-encoding-with-pyspark [11 September 2022].\n",
    "* Stack Overflow. How to extract model hyper-parameters from spark.ml in PySpark? [Online]. Available from: https://stackoverflow.com/questions/36697304/how-to-extract-model-hyper-parameters-from-spark-ml-in-pyspark [11 September 2022]."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
